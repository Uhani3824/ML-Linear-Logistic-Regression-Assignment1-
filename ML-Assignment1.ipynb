{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517c5021-f9de-42d6-a348-44aeb908c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe797722-b321-405a-8f1e-9dd807f5a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "#loading training dataset\n",
    "train_data=np.loadtxt('train-MLAssignment1.csv',delimiter=',',skiprows=1)\n",
    "\n",
    "#loading testing dataset\n",
    "test_data=np.loadtxt('test-MLAssignment1.csv',delimiter=',',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d9097e-297d-49b6-9fd7-757a4d82f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating features and labels\n",
    "X_train=train_data[:,1:]\n",
    "Y_train=train_data[:,0]\n",
    "\n",
    "X_test=test_data[:,1:]\n",
    "Y_test=test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d04c73-4848-41df-82d4-1831d96d1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler =MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52986e89-0d01-4b0d-8ed1-c804710597fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling train and test datasets\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b651fbe5-ddbc-482b-9420-88217d8cf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_features=X_train_scaled.shape[1]\n",
    "\n",
    "weights=np.zeros(no_of_features)\n",
    "bias=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04b377c-c973-4589-acef-7042508e9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y(predicted y)=W.X+b\n",
    "def predict_price(X,weights,bias):\n",
    "    return np.dot(X,weights)+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce93842b-a8f1-44d9-be15-e86e5c68598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function=1/m(predicted-original)^2\n",
    "def calculate_cost(X,y,weights,bias):\n",
    "    m=X.shape[0]\n",
    "    predictions=predict_price(X,weights,bias)\n",
    "    cost=(1/(2*m))*np.sum((predictions-y)**2)\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1751fd2e-1737-4771-a074-e1c05e0952c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,weights,bias,learning_rate):\n",
    "    m= X.shape[0]\n",
    "    predictions=predict_price(X,weights,bias)\n",
    "    #calculating gradients\n",
    "    dW = (1 / m) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / m) * np.sum(predictions - y)\n",
    "    #updating weights and bias\n",
    "    weights -= learning_rate * dW\n",
    "    bias -= learning_rate * db\n",
    "    return weights,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa3e85f1-4089-48bf-945a-cb5e513bdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "iterations=1000\n",
    "\n",
    "# Training loop\n",
    "for i in range(iterations):\n",
    "    weights, bias = gradient_descent(X_train_scaled, Y_train, weights, bias, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e004f187-95d9-4670-a8bc-c78fd3024637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 26690568587.087746\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_test_predicted = predict_price(X_test_scaled, weights, bias)\n",
    "\n",
    "# Calculate MSE for the test set\n",
    "test_mse = mean_squared_error(Y_test, Y_test_predicted)\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b34963c-1e67-4115-a350-7e03125b3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "from numpy.linalg import solve\n",
    "\n",
    "x_data= np.array([0,2,4,6,8,10])\n",
    "y_data= np.array([0, 2.90, 14.8, 39.6, 74.3, 119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c477c3a-fb16-4888-b483-608241eecf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "#C=inverse of A.B\n",
    "A=np.vander(x_data,6)\n",
    "interpolation=solve(A,y_data)\n",
    "\n",
    "def interpolating_polynomial(x):\n",
    "    C=interpolation\n",
    "    return C[0] + C[1]*x + C[2]*x**2 + C[3]*x**3 + C[4]*x**4 + C[5]*x**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5d4aed-d3e2-4350-a88c-60932f2ea74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_reshaped_data=x_data.reshape(-1,1)\n",
    "polynomial = PolynomialFeatures(degree=2)\n",
    "x_polynomial = polynomial.fit_transform(x_reshaped_data)\n",
    "model = LinearRegression().fit(x_polynomial, y_data)\n",
    "intercept_regression = model.intercept_\n",
    "coefficients_regression = model.coef_\n",
    "\n",
    "def regression_polynomial(x):\n",
    "    return intercept_regression + coefficients_regression[1]*x + coefficients_regression[2]*x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07fd9732-537f-4784-befe-9ded9e2fc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_reshaped_data=x_data.reshape(-1,1)\n",
    "polynomial = PolynomialFeatures(degree=2)\n",
    "x_polynomial = polynomial.fit_transform(x_reshaped_data)\n",
    "model = LinearRegression().fit(x_polynomial, y_data)\n",
    "intercept_regression = model.intercept_\n",
    "coefficients_regression = model.coef_\n",
    "\n",
    "def regression_polynomial(x):\n",
    "    return intercept_regression + coefficients_regression[1]*x + coefficients_regression[2]*x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509b2e95-777b-4043-9191-cf75e1c484f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated force at 750 ft/sec using Interpolating Polynomial: 4951.088541666712 (100 lb)\n",
      "Estimated force at 750 ft/sec using Quadratic Regression Polynomial: 64.29308035714286 (100 lb)\n"
     ]
    }
   ],
   "source": [
    "# Calculating force estimates at 750 ft/sec for both models\n",
    "velocity_to_estimate = 7.5  # 750 ft/sec in terms of given scale (100 ft/sec units)\n",
    "force_interpolating = interpolating_polynomial(velocity_to_estimate)\n",
    "force_regression = regression_polynomial(velocity_to_estimate)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"Estimated force at 750 ft/sec using Interpolating Polynomial: {force_interpolating} (100 lb)\")\n",
    "print(f\"Estimated force at 750 ft/sec using Quadratic Regression Polynomial: {force_regression} (100 lb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe36607d-5746-4833-ba36-4eae969c18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differences of both approaches:\n",
    "#Each approach has its own strengths, depending on whether the goal is precision at given points or generalizability across a range of values.\n",
    "#Interpolating Polynomial is ideal when an exact match to each data point is required, but it can become unstable and unreliable for extrapolation.\n",
    "#Quadratic Regression Polynomial provides a generalized trend that balances accuracy with stability, making it useful for predictions and minimizing overall error, though it sacrifices exactness at individual points.\n",
    "#Results of both approaches:\n",
    "#The interpolating polynomial provides a very high estimate due to overfitting and oscillations from its high degree, making it unreliable for extrapolation. In contrast, the quadratic regression polynomial captures the general trend, providing a more stable and reasonable estimate outside the data range.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ceb81d-f81d-44ae-b21b-4bf1acbd7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "#loading training dataset\n",
    "train_data=np.loadtxt('trainlr-MLAssignment1.csv',delimiter=',',skiprows=1)\n",
    "\n",
    "#loading testing dataset\n",
    "test_data=np.loadtxt('testlr-MLAssignment1.csv',delimiter=',',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79aa871f-65b0-4987-b792-df4c9c88b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features and target labels from training data\n",
    "X_train = train_data[:, :-1]  \n",
    "y_train = train_data[:, -1]  \n",
    "\n",
    "# Extracting features and target labels from testing data\n",
    "X_test = test_data[:, :-1]    \n",
    "y_test = test_data[:, -1]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12a6919-0705-49bb-80ed-a46f776a6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1\n",
    "#f(z)=1/1+e^-z\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce05d1ff-610d-4539-bc88-4d57140e01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#J=-y.log(f(x))-(1-y)log(1-f(x))\n",
    "def compute_loss(y, y_pred):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286cf311-8f0a-4317-b90f-2d8725589c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    # Initializing weights and bias\n",
    "    weights = np.zeros(X.shape[1])  \n",
    "    bias = 0\n",
    "    m = len(y)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Calculating predictions\n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_pred = sigmoid(z)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        error = y_pred - y\n",
    "        gradient_weights = np.dot(X.T, error) / m\n",
    "        gradient_intercept = np.mean(error)\n",
    "        \n",
    "        # Updating weights and bias\n",
    "        weights -= learning_rate * gradient_weights\n",
    "        bias -= learning_rate * gradient_intercept\n",
    "    \n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f518d1b6-6e65-4ebc-9b89-68efb891f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "weights, bias = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a14d6271-629d-406c-84d1-8fbf5da897be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    return sigmoid(np.dot(X, weights) + bias) >= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43bf6975-d74b-4712-a3c9-074ac7efcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c0f179-c36b-430a-9d2d-1c75e0b5e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model's accuracy on test data: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test data\n",
    "y_pred_test = predict(X_test, weights, bias)\n",
    "\n",
    "# Calculating accuracy on the test data\n",
    "test_accuracy = accuracy(y_test, y_pred_test)\n",
    "print(f\"Linear model's accuracy on test data: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e49de33-2882-4885-89a6-9b957c13e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2\n",
    "#Polynomial feature transformation\n",
    "def add_polynomial_features(X, degree=2):\n",
    "    from itertools import combinations_with_replacement\n",
    "    poly_features = [X]\n",
    "    for d in range(2, degree + 1):\n",
    "        for comb in combinations_with_replacement(range(X.shape[1]), d):\n",
    "            poly_feature = np.prod([X[:, i] for i in comb], axis=0)\n",
    "            poly_features.append(poly_feature[:, np.newaxis])\n",
    "    return np.hstack(poly_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "212d3b94-ebf0-46b6-98b9-9c4505169a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-linear model's test accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "#Transforming features \n",
    "X_train_poly = add_polynomial_features(X_train, degree=3)\n",
    "X_test_poly = add_polynomial_features(X_test, degree=3)\n",
    "#Training non-linear model\n",
    "weights_poly, intercept_poly = gradient_descent(X_train_poly, y_train)\n",
    "#Testing\n",
    "y_pred_test_poly = predict(X_test_poly, weights_poly, intercept_poly)\n",
    "test_accuracy_poly = accuracy(y_test, y_pred_test_poly)\n",
    "print(f\"Non-linear model's test accuracy: {test_accuracy_poly:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07bb170b-7589-4432-8fe6-9281f7f8405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison:\n",
    "#The linear and non-linear logistic regression models achieved the same accuracy of 75%, indicating that the data may be linearly separable, or \n",
    "#the polynomial transformation in the non-linear model did not significantly improve the fit. The linear model is preferable due to its simplicity, \n",
    "#efficiency, and interpretability, while the non-linear model, though more flexible, did not offer additional benefits in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
